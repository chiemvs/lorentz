{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972f1eea-16f4-47dd-996d-c4cb690e6df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 08:22:33.199966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-09 08:22:33.200000: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from cnn_training import generate_climprob_inputs, ModelRegistry, construct_climdev_cnn, DEFAULT_FIT, DEFAULT_COMPILE, DEFAULT_CONSTRUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d92c158-6b08-42f9-96ad-9b9e0a0a5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratchdir = Path('/scratch/')\n",
    "experiment_name = 'trial5_ensmean_something'\n",
    "ensmean = True\n",
    "varlist = ['tcw']\n",
    "season = None\n",
    "patchsize = 40 # If single value then the number of degrees for square latlon patch, otherewise supply tuple of ((latmin,latmax),(lonmin,lonmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d79afb4-8a72-4d67-b9e2-5b589bc648ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of input and target data\n",
    "train_inputs = np.load(scratchdir / f'{experiment_name}.training_inputs.npy') # (nsamples,nlat,nlon,nchannels), if ensmean then channels is just the number of variables, nlat & nlon depend on patch size\n",
    "train_target = np.load(scratchdir / f'{experiment_name}.training_terciles.npy') # Spatial average 4-week rainfall classified into terciles, one hot encoded (low, mid, high)\n",
    "train_timestamps = pd.read_hdf(scratchdir / f'{experiment_name}.training_timestamps.h5')\n",
    "\n",
    "test_inputs = np.load(scratchdir / f'{experiment_name}.testing_inputs.npy') # these are the forecasts that have been kept separate\n",
    "test_target = np.load(scratchdir / f'{experiment_name}.testing_terciles.npy')\n",
    "test_timestamps = pd.read_hdf(scratchdir / f'{experiment_name}.testing_timestamps.h5')\n",
    "\n",
    "full_inputs = np.concatenate([train_inputs, test_inputs], axis = 0) # Stacking along valid_time/sample dimension\n",
    "full_target = np.concatenate([train_target, test_target], axis = 0) # Stacking along valid_time/sample dimension\n",
    "full_timestamps = pd.concat([train_timestamps, test_timestamps])\n",
    "\n",
    "# Now we want to prepare input to the second branch of the neural network (the base probabilities for each of the classes)\n",
    "n_classes = full_target.shape[-1]\n",
    "full_clim_logprobs = generate_climprob_inputs(full_inputs, climprobs = full_target.mean(axis = 0)) # 3 classes are assumed to be equiprobable terciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0dcd4b-f980-466a-9be2-c68103ff1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is the moment to choose your hyperparameters\n",
    "# We supply them as kwargs to the structure that will initialize and contain all models\n",
    "registry = ModelRegistry(xdata = [full_inputs, full_clim_logprobs],\n",
    "        ydata = full_target,\n",
    "        timestamps = full_timestamps.index,\n",
    "        compile_kwargs = DEFAULT_COMPILE, construct_kwargs = DEFAULT_CONSTRUCT, fit_kwargs = DEFAULT_FIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2384857d-9cf2-4048-970d-1a79388527e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2020\n",
      "validation [2005, 2010, 2015]\n",
      "training Int64Index([2000, 2001, 2002, 2003, 2004, 2006, 2007, 2008, 2009, 2011, 2012,\n",
      "            2013, 2014, 2016, 2017, 2018, 2019, 2021],\n",
      "           dtype='int64', name='valid_time')\n",
      "0\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 08:24:35.901848: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-09 08:24:35.901882: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-09 08:24:35.901903: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyternoteboo): /proc/driver/nvidia/version does not exist\n",
      "2022-09-09 08:24:35.902078: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 11ms/step - loss: 0.6059 - accuracy: 0.6737 - val_loss: 0.6693 - val_accuracy: 0.5849\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5994 - accuracy: 0.6737 - val_loss: 0.6667 - val_accuracy: 0.5849\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5957 - accuracy: 0.6737 - val_loss: 0.6619 - val_accuracy: 0.5849\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5930 - accuracy: 0.6737 - val_loss: 0.6580 - val_accuracy: 0.5849\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5910 - accuracy: 0.6770 - val_loss: 0.6564 - val_accuracy: 0.5849\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5896 - accuracy: 0.6759 - val_loss: 0.6555 - val_accuracy: 0.5849\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5887 - accuracy: 0.6837 - val_loss: 0.6533 - val_accuracy: 0.5912\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5878 - accuracy: 0.6915 - val_loss: 0.6498 - val_accuracy: 0.5975\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5867 - accuracy: 0.6937 - val_loss: 0.6497 - val_accuracy: 0.6038\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.5862 - accuracy: 0.6915 - val_loss: 0.6462 - val_accuracy: 0.6226\n"
     ]
    }
   ],
   "source": [
    "# Select test and validation years, run model\n",
    "test_year = 2020\n",
    "validation_years = [2005, 2010, 2015]\n",
    "years = full_timestamps.index.year.unique()\n",
    "remaining_years = years.drop(test_year).sort_values() # sort to make sure that the next leaving out is blockwise\n",
    "training_years = years.drop(test_year).drop(validation_years)\n",
    "print('test', test_year)\n",
    "print('validation', validation_years)\n",
    "print('training', training_years)\n",
    "test_indices = np.where(full_timestamps.index.year == test_year)[0] # From boolean to numeric index\n",
    "val_indices = np.where(full_timestamps.index.year.map(lambda y: y in validation_years))[0]\n",
    "train_indices = np.where(full_timestamps.index.year.map(lambda y: y in training_years))[0]\n",
    "modelindex = registry.initialize_untrained_model(train_indices = train_indices, val_indices = val_indices, test_indices = test_indices)\n",
    "print(modelindex)\n",
    "registry.train_model(modelindex = modelindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "809c41ee-076c-4b4a-8e13-b252c7fe3e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valid_time\n",
       "2020-03-06   2020-03-06\n",
       "2020-04-03   2020-04-03\n",
       "2020-05-01   2020-05-01\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_date_indices = range(7,17,4)\n",
    "test_timestamps[test_date_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39142e41-3e04-49a3-bba7-ef3970eedd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_sample in test_date_indices:\n",
    "    yyyy = test_timestamps[n_sample].year\n",
    "    mm = test_timestamps[n_sample].month\n",
    "    dd = test_timestamps[n_sample].day\n",
    "    np.save(f'dianna/test_{yyyy}/test_inputs_{mm}-{dd}.npy', test_inputs[n_sample])\n",
    "    np.save(f'dianna/test_{yyyy}/test_target_{mm}-{dd}.npy', test_target[n_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbd55f56-0882-4a3b-bb6b-a08369048d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dianna/test_2020/pre-trained/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dianna/test_2020/pre-trained/assets\n"
     ]
    }
   ],
   "source": [
    "registry.registry[-1].save(f'dianna/test_{yyyy}/pre-trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fce8b38-c1df-4629-8cf0-078542a0d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "climprobs = train_target.mean(axis = 0)\n",
    "np.save(f'dianna/test_{yyyy}/climprobs.npy', climprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdb225-6455-4a2e-ab21-1229cec9a37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
